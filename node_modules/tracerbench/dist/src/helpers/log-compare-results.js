"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
const chalk_1 = require("chalk");
const create_consumable_html_1 = require("./create-consumable-html");
const stats_1 = require("./statistics/stats");
const table_1 = require("./table");
const command_config_1 = require("../command-config");
const utils_1 = require("./utils");
/**
 * If fidelity is at acceptable number, return true if any of the phase results were significant
 *
 * @param fidelity - Use this to determine if the sample count is too low
 * @param benchmarkIsSigArray - Array of strings of either "Yes" or "No" from TBTable
 * @param phaseIsSigArray - Array of strings of either "Yes" or "No" from TBTable
 */
function anyResultsSignificant(fidelity, benchmarkIsSigArray, phaseIsSigArray) {
    // if fidelity !== 'test'
    if (fidelity > command_config_1.fidelityLookup.test) {
        return benchmarkIsSigArray.includes(true) || phaseIsSigArray.includes(true);
    }
    return false;
}
exports.anyResultsSignificant = anyResultsSignificant;
/**
 * If any phase of the experiment has regressed slower beyond the threshold limit returns false; otherwise true
 *
 * @param regressionThreshold - Positive number in milliseconds the experiment has regressed slower eg 100
 * @param benchmarkTableEstimatorDeltas - Array of Estimator Deltas for the Benchmark Table
 * @param phaseTableEstimatorDeltas - Array of Estimator Deltas for the Phase Table
 */
function allBelowRegressionThreshold(regressionThreshold, benchmarkTableEstimatorDeltas, phaseTableEstimatorDeltas) {
    function isBelowThreshold(n) {
        const limit = regressionThreshold;
        // if the delta is a negative number and abs(delta) greater than threshold return false
        return n < 0 && Math.abs(n) > limit ? false : true;
    }
    if (typeof regressionThreshold === 'number') {
        // concat estimator deltas from all phases
        const deltas = benchmarkTableEstimatorDeltas.concat(phaseTableEstimatorDeltas);
        // if the experiment is slower beyond the threshold return false;
        return deltas.every(isBelowThreshold);
    }
    return true;
}
exports.allBelowRegressionThreshold = allBelowRegressionThreshold;
/**
 * Output meta data about the benchmark run and FYI messages to the user.
 *
 * @param cli - This is expected to be a "compare" Command instance
 * @param cliFlags - This is expected to be CLI flags from the "compare" command
 * @param isBelowRegressionThreshold - Boolean indicating if there were any deltas below "regressionThreshold" flag
 */
function outputRunMetaMessagesAndWarnings(cli, cliFlags, isBelowRegressionThreshold) {
    const { fidelity, regressionThreshold } = cliFlags;
    const LOW_FIDELITY_WARNING = 'The fidelity setting was set below the recommended for a viable result. Rerun TracerBench with at least "fidelity=low"';
    if (fidelity < 10) {
        cli.log(`\n${utils_1.chalkScheme.blackBgYellow(`    ${utils_1.chalkScheme.white('WARNING')}    `)} ${utils_1.chalkScheme.warning(` ${LOW_FIDELITY_WARNING}`)}\n`);
    }
    if (!isBelowRegressionThreshold) {
        cli.log(`\n${utils_1.chalkScheme.blackBgRed(`    ${utils_1.chalkScheme.white('!! ALERT')}    `)} ${chalk_1.default.red(` Regression found exceeding the set regression threshold of ${regressionThreshold}ms`)}\n`);
    }
}
exports.outputRunMetaMessagesAndWarnings = outputRunMetaMessagesAndWarnings;
/**
 * Generate the summary section for the results.
 *
 * For each phase, color the significance appropriately by the HL estimated difference. Red for regression, green for
 * improvement. Color with monotone if not significant.
 *
 * @param cli - This is expected to be a "compare" Command instance
 * @param phaseResultsFormatted - Array of results from calling formatPhaseData
 */
function outputSummaryReport(cli, phaseResultsFormatted) {
    cli.log(`\n${utils_1.chalkScheme.blackBgBlue(`    ${utils_1.chalkScheme.white('Benchmark Results Summary')}    `)}`);
    cli.log(`\n${chalk_1.default.red('Red')} color means there was a regression.`);
    cli.log(`${chalk_1.default.green('Green')} color means there was an improvement.\n`);
    phaseResultsFormatted.forEach(phaseData => {
        const { phase, hlDiff, isSignificant, ciMin, ciMax } = phaseData;
        let msg = `${chalk_1.default.bold(phase)} phase `;
        if (isSignificant && Math.abs(hlDiff)) {
            let coloredDiff;
            msg += 'estimated difference ';
            if (hlDiff < 0) {
                coloredDiff = chalk_1.default.red(`+${Math.abs(hlDiff)}ms [${ciMax * -1}ms to ${ciMin * -1}ms]`);
            }
            else {
                coloredDiff = chalk_1.default.green(`-${Math.abs(hlDiff)}ms [${ciMax * -1}ms to ${ciMin * -1}ms]`);
            }
            msg += `${coloredDiff}`;
        }
        else {
            msg += `${chalk_1.default.grey('no difference')}`;
        }
        cli.log(msg);
    });
}
exports.outputSummaryReport = outputSummaryReport;
/**
 * Return the trimmed compare results in JSON format
 *
 * This is propogated as the default return all the way up to the Compare command directly
 * without the need for the legacy --json flag
 *
 * @param benchmarkTableData - ICompareJSONResult[] from instantiated TBTable#getData() for the top level duration
 * @param phaseTableData - ICompareJSONResult[] from instantiated TBTable#getData() for all sub phases of the top level duration
 * @param areResultsSignificant - A culled boolean if any results are significant this is truthy
 * @param isBelowRegressionThreshold - A culled boolean to check if all results are below the config regression threshold
 * @return jsonResults - A JSON.stringified return of the trimmed compare results
 */
function outputJSONResults(benchmarkTableData, phaseTableData, areResultsSignificant, isBelowRegressionThreshold) {
    const jsonResults = {
        benchmarkTableData,
        phaseTableData,
        areResultsSignificant,
        isBelowRegressionThreshold,
    };
    return JSON.stringify(jsonResults);
}
exports.outputJSONResults = outputJSONResults;
/**
 * Collect and analyze the data for the different phases for the experiment and control set and output the result to the console.
 *
 * @param results - This is expected to be generated from tracerbench core's runner. Containing the dataset for experiment and control
 * @param flags - This is expected to be CLI flags from the "compare" command
 * @param cli - This is expected to be a "compare" Command instance
 */
async function logCompareResults(results, flags, cli) {
    const { fidelity } = flags;
    const benchmarkTable = new table_1.default('Initial Render');
    const phaseTable = new table_1.default('Sub Phase of Duration');
    const controlData = results.find(element => {
        return element.set === 'control';
    });
    const experimentData = results.find(element => {
        return element.set === 'experiment';
    });
    const valuesByPhaseControl = create_consumable_html_1.bucketPhaseValues(controlData.samples);
    const valuesByPhaseExperiment = create_consumable_html_1.bucketPhaseValues(experimentData.samples);
    const subPhases = Object.keys(valuesByPhaseControl).filter(k => k !== create_consumable_html_1.PAGE_LOAD_TIME);
    const phaseResultsFormatted = [];
    const durationStats = new stats_1.Stats({
        control: valuesByPhaseControl.duration,
        experiment: valuesByPhaseExperiment.duration,
        name: 'duration',
    });
    benchmarkTable.display.push(durationStats);
    // @ts-ignore
    phaseResultsFormatted.push(create_consumable_html_1.formatPhaseData(valuesByPhaseControl[create_consumable_html_1.PAGE_LOAD_TIME], valuesByPhaseExperiment[create_consumable_html_1.PAGE_LOAD_TIME], create_consumable_html_1.PAGE_LOAD_TIME));
    subPhases.forEach(phase => {
        phaseTable.display.push(new stats_1.Stats({
            control: valuesByPhaseControl[phase],
            experiment: valuesByPhaseExperiment[phase],
            name: phase,
        }));
        // @ts-ignore
        phaseResultsFormatted.push(create_consumable_html_1.formatPhaseData(valuesByPhaseControl[phase], valuesByPhaseExperiment[phase], phase));
    });
    let isBelowRegressionThreshold = true;
    const benchmarkTableData = benchmarkTable.getData();
    const phaseTableData = phaseTable.getData();
    const areResultsSignificant = anyResultsSignificant(fidelity, benchmarkTable.isSigArray, phaseTable.isSigArray);
    if (areResultsSignificant) {
        isBelowRegressionThreshold = allBelowRegressionThreshold(fidelity, benchmarkTable.estimatorDeltas, phaseTable.estimatorDeltas);
    }
    cli.log(`\n\n${benchmarkTable.render()}`);
    cli.log(`\n\n${phaseTable.render()}`);
    outputRunMetaMessagesAndWarnings(cli, flags, isBelowRegressionThreshold);
    outputSummaryReport(cli, phaseResultsFormatted);
    return outputJSONResults(benchmarkTableData, phaseTableData, areResultsSignificant, isBelowRegressionThreshold);
}
exports.logCompareResults = logCompareResults;
//# sourceMappingURL=log-compare-results.js.map